{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing"
      ],
      "metadata": {
        "id": "98fgNw-WyogE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download NLTK libraries"
      ],
      "metadata": {
        "id": "TqUw3k9oD-jU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "HSn0Euzs_sAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Punction, sentence tokenizer, pos tagging, lemmatization process"
      ],
      "metadata": {
        "id": "GLkOy_yMEEAj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sgjDG3eUyQpF"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def punctionProcess(text):\n",
        "    out = str(text).maketrans('','', string.punctuation)\n",
        "    result = str(text).translate(out)\n",
        "    \n",
        "    return result\n",
        "\n",
        "#punktTokenizer for sentence detection\n",
        "def punktSentenceTokenizer(paragraph):\n",
        "    sent_tokenizer = nltk.tokenize.PunktSentenceTokenizer()\n",
        "    sentences = sent_tokenizer.tokenize(paragraph)\n",
        "    updateSentence = solveSentenceTokenizer(sentences)\n",
        "    if len(updateSentence) == 0:\n",
        "        updateSentence = sentences\n",
        "\n",
        "    return updateSentence\n",
        "\n",
        "\n",
        "#POS Tagging\n",
        "def posTag(text):\n",
        "    txt = word_tokenize(text)\n",
        "    pos = nltk.pos_tag(txt)\n",
        "\n",
        "    return pos\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Lemmatize with POS Tag\n",
        "from nltk.corpus import wordnet\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "#Lemmatization process\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "def lemmaProcess(sentence):       \n",
        "    input_str=word_tokenize(sentence)\n",
        "    txt = ''\n",
        "    for word in input_str:\n",
        "        txt += lemmatizer.lemmatize(word, get_wordnet_pos(word)) + ' ' #, get_wordnet_pos(word)\n",
        "    sentence = str(txt).strip()\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "#developed solve for sentence tokenizer error\n",
        "def solveSentenceTokenizer(sentences):\n",
        "    firstStage = []\n",
        "    ind = 0\n",
        "    indList = []\n",
        "    last = []\n",
        "    #If next sentence first word's first letter is lower or numeric char, combine with sentences\n",
        "    s = ''\n",
        "    for sentence in sentences:\n",
        "        if ind+1 < len(sentences):\n",
        "            words = word_tokenize(sentences[ind+1])\n",
        "            word = words[0]\n",
        "            if str(word).islower() == True or str(word).isnumeric() == True:\n",
        "                firstStage.append(sentence+ ' ' + sentences[ind+1])\n",
        "                s += sentence+ ' ' + sentences[ind+1]\n",
        "                last.append(ind+1)\n",
        "                if ind+1 - last[len(last)-2] == 1:\n",
        "                    sent = firstStage[len(firstStage)-2]\n",
        "                    firstStage.remove(firstStage[len(firstStage)-2])\n",
        "                    firstStage.remove(firstStage[len(firstStage)-1])\n",
        "                    sent += ' ' + sentences[ind+1]\n",
        "                    firstStage.append(sent)\n",
        "                    s += sent\n",
        "            else:\n",
        "                firstStage.append(sentence)  \n",
        "        else:\n",
        "                firstStage.append(sentence) \n",
        "        ind += 1\n",
        "    for ind in range(0, len(firstStage)-1):\n",
        "        if str(firstStage[ind]).find(firstStage[ind+1]) != -1:\n",
        "            indList.append(ind+1)\n",
        "    indList.sort(reverse=True)\n",
        "    for ind in indList:\n",
        "        firstStage.remove(firstStage[ind])\n",
        "    #If sentence containing max 2 word, combines with next sentences\n",
        "    secondStage = []\n",
        "    ind = 0\n",
        "    for sentence in firstStage:\n",
        "        words = word_tokenize(sentence) \n",
        "        if len(words) < 3:\n",
        "            if  ind+1 < len(firstStage):\n",
        "                secondStage.append(sentence + ' ' + firstStage[ind+1])\n",
        "        else:\n",
        "            if len(secondStage) > 0:\n",
        "                if str(secondStage[len(secondStage)-1]).find(sentence) == -1:\n",
        "                    secondStage.append(sentence)\n",
        "            else:\n",
        "                secondStage.append(sentence)\n",
        "        ind += 1\n",
        "    #Previous word is uppercase, but last word's first character is uppercase with max 3 length => combine with next sentences \n",
        "    lastSentences = []\n",
        "    ind = 0\n",
        "    for update in secondStage:\n",
        "        words = word_tokenize(update) \n",
        "        if len(words) >= 2:\n",
        "            word = words[len(words)-2]\n",
        "            if len(lastSentences) != 0:\n",
        "                if str(lastSentences[len(lastSentences)-1]).find(update) != -1:\n",
        "                    ind += 1\n",
        "                    continue\n",
        "            if str(word[0]).isupper() == True and len(word) < 4 and len(words) >= 3:\n",
        "                word = words[len(words)-3]\n",
        "                if str(word[0]).islower() == True and len(secondStage) > ind+1:\n",
        "                    lastSentences.append(update + ' ' + secondStage[ind+1])\n",
        "                else:\n",
        "                    lastSentences.append(update)\n",
        "            else:\n",
        "                lastSentences.append(update)\n",
        "        ind += 1\n",
        "    return lastSentences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named Entity Recognition process"
      ],
      "metadata": {
        "id": "q_nIa7jXzCmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "#NER process for preprocessing\n",
        "def NerRecognition(article, answ_list):\n",
        "    article = nlp(article)\n",
        "    \n",
        "    items = [x.text for x in article.ents]   \n",
        "    ner_list = Counter(items).keys()\n",
        "    sw = stopwords.words('english') \n",
        "    for ner in ner_list:\n",
        "        if str(ner).find('\\'s') != -1:\n",
        "            indx = str(ner).index('\\'s')\n",
        "            ner = ner[:indx]\n",
        "        if str(ner).find(' ') != -1:                  \n",
        "            ner_ = str(ner).split(' ')\n",
        "            islow = False\n",
        "            if str(ner_[len(ner_)-1]).islower() == True:\n",
        "                islow = True\n",
        "            try:\n",
        "                ind = sw.index(ner_[0])\n",
        "            except:\n",
        "                uri = \"\"\n",
        "                for i in ner_:\n",
        "                    if islow == False:\n",
        "                        try:\n",
        "                            try:\n",
        "                                ner_list.index(i)\n",
        "                            except:\n",
        "                                ind = answ_list.index(i)\n",
        "                                answ_list.remove(str(i).strip())\n",
        "                            uri += str(i)  + \"_\" \n",
        "                        except:\n",
        "                            uri += str(i) + \"_\"                            \n",
        "                if uri != \"\": answ_list.append(str(uri[:-1]))\n",
        "        else:\n",
        "            for ans in answ_list:\n",
        "                if ans.find(ner) != -1:\n",
        "                    answ_list.remove(ans)\n",
        "                    answ_list.append(ner)\n",
        "                    break\n",
        "    return answ_list"
      ],
      "metadata": {
        "id": "XiE34ceczBsS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo Processes"
      ],
      "metadata": {
        "id": "k4uK4wkGzgJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from spacy import displacy\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "qrgsb1cKzkjz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer type detection function"
      ],
      "metadata": {
        "id": "AfX9CSj_EWDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def findAnswerType(pos_txt):\n",
        "\tansType = ''\n",
        "\tif pos_txt == 'people' or pos_txt == 'person':\n",
        "\t\tansType = 'PERSON'\n",
        "\tif pos_txt == 'nationality' or pos_txt == 'religion' or pos_txt == 'politic':\n",
        "\t\tansType = 'NORP'\n",
        "\tif pos_txt == 'country' or pos_txt == 'city' or pos_txt == 'state':\n",
        "\t\tansType = 'GPE'\n",
        "\tif pos_txt == 'company' or pos_txt == 'agency' or pos_txt == 'institution':\n",
        "\t\tansType = 'ORG'\n",
        "\tif pos_txt == 'building' or pos_txt == 'airport' or pos_txt == 'highway' or pos_txt == ' bridge':\n",
        "\t\tansType = 'FAC'\n",
        "\tif pos_txt == 'location' or pos_txt == 'range':\n",
        "\t\tansType = 'LOC'\n",
        "\tif pos_txt == 'hurricane' or pos_txt == 'battle' or pos_txt == 'war':\n",
        "\t\tansType = 'EVENT'\n",
        "\tif pos_txt == 'song' or pos_txt == 'title':\n",
        "\t\tansType = 'WORK_OF_ART'\n",
        "\tif pos_txt == 'language':\n",
        "\t\tansType = 'LANGUAGE'\n",
        "\tif pos_txt == 'time' or pos_txt == 'date' or pos_txt == 'period' or pos_txt == 'duration' or pos_txt == 'year' or pos_txt == 'month' or pos_txt == 'day':\n",
        "\t\tansType = 'DATE'\n",
        "\tif pos_txt == 'percentage':\n",
        "\t\tansType = 'PERCENT'\n",
        "\tif pos_txt == 'measurement' or pos_txt == 'weight' or pos_txt == 'distance':\n",
        "\t\tansType = 'QUANTITY'\n",
        "\t\t\n",
        "\treturn ansType"
      ],
      "metadata": {
        "id": "RkUwyAZpz_Uq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define RNP methods"
      ],
      "metadata": {
        "id": "BrctRKOD1VQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def demo(related_sentence, answerTerm, questionSearch, quesNormal, totalFindAnswer, methodNo, isLemmatization):\n",
        "    processes = ''\n",
        "    sentence = related_sentence\n",
        "    try:         \n",
        "        question = questionSearch.split(',')\n",
        "        if isLemmatization == True:          \n",
        "            sentence = lemmaProcess(sentence)\n",
        "            quesNormal = lemmaProcess(quesNormal)\n",
        "            answerTerm = lemmaProcess(answerTerm)\n",
        "        \n",
        "        sentenceTerm = []\n",
        "        #Remove&Compare method\n",
        "        if methodNo == 1:           \n",
        "            question = punctionProcess(quesNormal)  \n",
        "            question = question.lower().strip().split(' ')\n",
        "            sentence = punctionProcess(sentence)  \n",
        "            sentence = sentence.lower().strip().split(' ')\n",
        "            answer = punctionProcess(answerTerm)   \n",
        "            answer = answer.lower().strip().split(' ')\n",
        "            sw = stopwords.words('english')\n",
        "            processes += '\\nRemoving question words in sentence: ' \n",
        "            for ques in question:                \n",
        "                try: \n",
        "                    sentence.remove(ques)\n",
        "                    processes += ques + ','\n",
        "                except: \n",
        "                    for synset in wordnet.synsets(ques):\n",
        "                        cnt = False\n",
        "                        for syn in synset.lemmas():\n",
        "                            if str(related_sentence).find(syn.name()) != -1:\n",
        "                                try:\n",
        "                                    sentence.remove(str(syn.name()))\n",
        "                                    cnt = True\n",
        "                                    break\n",
        "                                except: continue\n",
        "                        if cnt == True:\n",
        "                            break\n",
        "                    continue\n",
        "            processes += '\\n\\nRemoving stopwords in sentence: '\n",
        "            for stop in sw:\n",
        "                try: \n",
        "                    sentence.remove(stop)\n",
        "                    processes += stop + ','                    \n",
        "                except: \n",
        "                    continue\n",
        "            processes += '\\n\\nRemoving stopwords in answer: '\n",
        "            for stop in sw:\n",
        "                try: \n",
        "                    answer.remove(stop)\n",
        "                    processes += stop + ',' \n",
        "                except: \n",
        "                    continue\n",
        "            if len(sentence) == len(answer):\n",
        "                sent = ''\n",
        "                ans = ''\n",
        "                for s in sentence:\n",
        "                    sent += s + ' '\n",
        "                for a in answer:\n",
        "                    ans += a + ' '\n",
        "                if sent.strip() == ans.strip():\n",
        "                    totalFindAnswer += 1\n",
        "\n",
        "        #Searching with NER method\n",
        "        if methodNo == 2:\n",
        "            text = str(quesNormal).lower()\n",
        "            ansType = ''\n",
        "            if text.find('who') != -1:\n",
        "                ansType = 'PERSON'\n",
        "            if text.find('when') != -1:\n",
        "                ansType = 'DATE'\n",
        "            if text.find('how many') != -1:\n",
        "                ansType = 'QUANTITY'\n",
        "            if text.find('how much') != -1:\n",
        "                ansType = 'QUANTITY, MONEY'\n",
        "            if text.find('where') != -1:\n",
        "                ansType = 'GPE'\n",
        "            pos = posTag(quesNormal.lower())\n",
        "            if len(pos)> 2:\n",
        "                for i in range(len(pos)-1):\n",
        "                    if pos[i][0] == 'what' or pos[i][0] == 'which' or pos[i][0] == 'where' or pos[i][0] == 'whose':\n",
        "                        ansType = findAnswerType(pos[i+1][0])\n",
        "                if ansType == '':\n",
        "                    for i in range(len(pos)-1):\n",
        "                        ansType = findAnswerType(pos[i+1][0])\n",
        "                if ansType != '':                   \n",
        "                    sentence = nlp(sentence)\n",
        "                    labels = [x.label_ for x in sentence.ents]\n",
        "                    sentences = [x for x in sentence.sents]\n",
        "                    processes += 'Question\\'s NER label: ' + ansType + '\\n\\n'\n",
        "                    processes += 'Related sentence\\'s NER labels:\\n' \n",
        "                    answer = ''\n",
        "                    ind = 0\n",
        "                    for k in nlp(str(sentences[0])).ents:\n",
        "                        processes += str(k) + ': ' + str(labels[ind]) + '\\n' \n",
        "                        ind += 1\n",
        "                    ind = 0\n",
        "                    for k in nlp(str(sentences[0])).ents:\n",
        "                        if ansType.find(labels[ind]) != -1 and text.find(str(k)) == -1:\n",
        "                            answer = str(k)\n",
        "                            break\n",
        "                        ind += 1\n",
        "                    if answer == answerTerm or answer == answerTerm.lower():\n",
        "                        totalFindAnswer += 1\n",
        "\n",
        "        #Searching with POS method\n",
        "        if methodNo == 3:\n",
        "            pos = posTag(quesNormal.lower())\n",
        "            ansType = ''\n",
        "            if len(pos) > 2:\n",
        "                for i in range(len(pos)-1):         \n",
        "                    if pos[i][1] == 'NNP' or pos[i][1] == 'WP$':\n",
        "                        if pos[i][0] == 'whose': ansType = \"NNP\"\n",
        "                    if pos[i][1] == 'WP' or pos[i][1] == 'WP':\n",
        "                        if pos[i][0] == 'who': ansType = \"NNP\"\n",
        "                        if pos[i][0] == 'what': ansType = \"NN\"\n",
        "                    if pos[i][1] == \"WRB\":\n",
        "                        if pos[i][0] == 'when': ansType = \"CD\"\n",
        "                        if pos[i][0] == 'how': \n",
        "                            if pos[i+1][1] == 'JJ': ansType = \"CD\"\n",
        "                            else: ansType = \"JJ,RB\"\n",
        "                        if pos[i][0] == 'where': ansType = \"NN\"\n",
        "                        if pos[i][0] == 'why': ansType = \"IN\"\n",
        "                    if pos[i][1] == \"WDT\":\n",
        "                        ansType = \"NN\"\n",
        "                    if ansType != '':\n",
        "                        break\n",
        "        \n",
        "                if ansType != '':\n",
        "                    processes += 'Question\\'s POS tag: ' + ansType + '\\n\\n'\n",
        "                    processes += 'Removing question terms in sentence: '\n",
        "                    question = quesNormal[:-1].split(' ')\n",
        "                    for ques in question:\n",
        "                        if sentence.find(ques) != -1:\n",
        "                            ind = sentence.index(ques)\n",
        "                            processes += ques + ','\n",
        "                            sentence = sentence[:ind-1] + sentence[ind+len(ques):]                 \n",
        "                    posSent = posTag(sentence.strip())\n",
        "                    processes += '\\nRelated sentence\\'s POS tags:\\n'\n",
        "                    ans = ''\n",
        "                    ind = 0\n",
        "                    for key, p in posSent:\n",
        "                        processes += str(key) + ': ' + str(p) + '\\n' \n",
        "                        ind += 1\n",
        "                    ind = 0\n",
        "                    for key, p in posSent:\n",
        "                        if ansType.find(p) != -1:\n",
        "                            ans += str(key) + ' '\n",
        "                            if ind+1 < len(posSent)-1:\n",
        "                                for k in range(ind+1,len(posSent)):\n",
        "                                    if ansType.find(str(posSent[k][1])) != -1:\n",
        "                                        ans += str(posSent[k][0]) + ' '\n",
        "                                    else: break\n",
        "                            break\n",
        "                        ind += 1\n",
        "                    if ans != '':\n",
        "                        if ans[:-1] == answerTerm or ans[:-1] == answerTerm.lower():\n",
        "                            totalFindAnswer += 1        \n",
        "\n",
        "    except Exception as e:\n",
        "        print('Error Answer detection: ' + str(e) + ' - method:' + str(methodNo))        \n",
        "    return totalFindAnswer, processes"
      ],
      "metadata": {
        "id": "SMh4J3Lf0GRx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def defineQuestionTerm(preProcess):\n",
        "    questionSearch = ''\n",
        "    for terms in preProcess:\n",
        "        for term in terms:\n",
        "            if str(term).find('_') != -1:\n",
        "                term = str(term).replace('_',' ')\n",
        "            questionSearch += term + ','\n",
        "    questionSearch = questionSearch[:-1]\n",
        "    return questionSearch"
      ],
      "metadata": {
        "id": "Z6vGPKbP5qlI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Search question words in sentences with question term percentage(QTP)"
      ],
      "metadata": {
        "id": "BT9L2ApmEv3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def searchWord(text,paragraph,lemma):\n",
        "    texts = []\n",
        "    textSearch = ''\n",
        "    path = ''\n",
        "    try:\n",
        "        if str(text).find(',') != -1:\n",
        "            texts = str(text).split(',')\n",
        "        else:\n",
        "            texts.append(text)  \n",
        "        par = paragraph          \n",
        "        sentences = punktSentenceTokenizer(par)\n",
        "        s_ind = 0\n",
        "        rateList = []\n",
        "        textList = []\n",
        "        for sentence in sentences:         \n",
        "            sent = punctionProcess(str(sentence))\n",
        "            if lemma == True:\n",
        "                sent = lemmaProcess(sent)\n",
        "                sentence = sent\n",
        "            c = 0\n",
        "            qaText = [] \n",
        "            qTerm = []\n",
        "            for i in range(0,len(texts)):\n",
        "                texts[i] = str(texts[i]).strip()\n",
        "                if str(sent.lower()).find(texts[i].lower()) != -1:\n",
        "                    c += 1\n",
        "                    qTerm.append(str(texts[i]))\n",
        "\n",
        "            if c != 0:\n",
        "                textSearch += 'Find question:' + str(qTerm) + ', QCount:' + str(c) + '\\nQuestion rate:%' + str(c/len(texts)*100) + '\\n' + str(s_ind+1) + '. sentence - ' + sentence + '\\n\\n'\n",
        "                rateList.append(c/len(texts)*100)\n",
        "                textList.append('Find question:' + str(qTerm) + ', QCount:' + str(c) + '\\nQuestion rate:%' + str(c/len(texts)*100) + '\\n' +  str(s_ind+1) + '. sentence - ' + sentence + '\\n\\n')\n",
        "            s_ind += 1\n",
        "        if len(rateList) != 0:\n",
        "            path += findPath(rateList,textList)\n",
        "    except Exception as e:\n",
        "        print('SearchWord:' + str(e) + ', text:' + str(texts))\n",
        "    return textSearch, path"
      ],
      "metadata": {
        "id": "uB56oRZN6ClP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sorted all sentence according to QTP value"
      ],
      "metadata": {
        "id": "TUUXhrECEh3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def findPath(rateList,textList):\n",
        "    sortList = []\n",
        "    sortTxt = ''\n",
        "    for rate in rateList:\n",
        "        value = max(rateList)\n",
        "        ind = rateList.index(value)\n",
        "        sortList.append(ind)\n",
        "        rateList[ind] = 0\n",
        "    for sort in sortList:\n",
        "        sortTxt += textList[sort] + '\\n'\n",
        "    return sortTxt"
      ],
      "metadata": {
        "id": "sSgzmDvx-sO1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selected candidate sentence"
      ],
      "metadata": {
        "id": "GpsWALFAEx5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def demoProcess(question, paragraph, answer, lemma):\n",
        "  question = punctionProcess(question)\n",
        "  X_list = word_tokenize(question) \n",
        "  preProcess = [X_list]\n",
        "   \n",
        "  questionSearch = defineQuestionTerm(preProcess)\n",
        "  processes = 'Question terms: ' + questionSearch + '\\n'\n",
        "  textSearch, path = searchWord(questionSearch, paragraph, lemma)\n",
        "  texts = path.split('\\n')\n",
        "  print('Sorted sentence: ' + str(texts))\n",
        "  txtInd = 0\n",
        "  sel_sentence = ''\n",
        "  for txt in texts:\n",
        "      if (str(txt.lower()).find(answer.lower()) != -1 or str(txt).find(answer) != -1):\n",
        "          sentence = str(txt).split('sentence - ')\n",
        "          sentence = str(sentence[1]).strip()\n",
        "          sel_sentence = str(sentence).replace('\\n', '').strip()\n",
        "          break\n",
        "\n",
        "  return questionSearch, sel_sentence"
      ],
      "metadata": {
        "id": "2IEblKi08SDn"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running Examples"
      ],
      "metadata": {
        "id": "JNm21O_XE6iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph_examples = 'Tarkan Tevetoğlu is a Turkish singer and songwriter. Tarkan was born on 17 October 1972 in Alzey, West Germany, to Neşe and Ali Tevetoğlu. The name Tarkan is said to originate from an ancient Turkic King. Tarkan\\'s first album, Yine Sensiz, was released on cassettes on 26 December 1992. Since the early 1990s, with the successful sales of his albums, he has been a prominent figure of pop music, recognized in Turkey and worldwide.'\n",
        "question_examples = ['Who is Tarkan Tevetoğlu?','When was Tarkan born?','Whose name does Tarkan originate from?']\n",
        "answer_examples = ['Turkish singer and songwriter', '17 October 1972', 'Turkic King']\n",
        "  \n",
        "lemma = False\n",
        "question = ''\n",
        "preProcess = '' \n",
        "answer = ''\n",
        "paragraph = paragraph_examples"
      ],
      "metadata": {
        "id": "N4ORd74Y0OXS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove & Compare example"
      ],
      "metadata": {
        "id": "-SScw1h7_Ise"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = question_examples[0]\n",
        "answer = answer_examples[0]\n",
        "response = answer_examples[0]\n",
        "answ = answer_examples[0].lower().strip().split(' ')\n",
        "sw = stopwords.words('english')\n",
        "for stop in sw:\n",
        "    try: \n",
        "        answ.remove(stop)                 \n",
        "    except: continue\n",
        "res = ''\n",
        "for ans in answ:\n",
        "    res += ans + ' '\n",
        "answer = res\n",
        "\n",
        "questionSearch, sel_sentence = demoProcess(question, paragraph, response, lemma)\n",
        "print('Candidate sentence: ' + sel_sentence)\n",
        "totalFindAnswer = 0\n",
        "findCount = totalFindAnswer\n",
        "totalFindAnswer, process = demo(sel_sentence, answer, questionSearch, question, totalFindAnswer, 1, lemma)\n",
        "if totalFindAnswer != findCount:\n",
        "  print('RC: Answer found successfully, answer: ' + answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gqx7W8IK_H69",
        "outputId": "b528a05b-80a7-47f5-8497-83b1c011a2b5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted sentence: [\"Find question:['is', 'Tarkan', 'Tevetoğlu'], QCount:3\", 'Question rate:%75.0', '1. sentence - Tarkan Tevetoğlu is a Turkish singer and songwriter.', '', '', \"Find question:['Tarkan', 'Tevetoğlu'], QCount:2\", 'Question rate:%50.0', '2. sentence - Tarkan was born on 17 October 1972 in Alzey, West Germany, to Neşe and Ali Tevetoğlu.', '', '', \"Find question:['is', 'Tarkan'], QCount:2\", 'Question rate:%50.0', '3. sentence - The name Tarkan is said to originate from an ancient Turkic King.', '', '', \"Find question:['Tarkan'], QCount:1\", 'Question rate:%25.0', \"4. sentence - Tarkan's first album, Yine Sensiz, was released on cassettes on 26 December 1992.\", '', '', \"Find question:['is'], QCount:1\", 'Question rate:%25.0', '5. sentence - Since the early 1990s, with the successful sales of his albums, he has been a prominent figure of pop music, recognized in Turkey and worldwide.', '', '', '']\n",
            "Candidate sentence: Tarkan Tevetoğlu is a Turkish singer and songwriter.\n",
            "RC: Answer found successfully, answer: turkish singer songwriter \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SNER example"
      ],
      "metadata": {
        "id": "PlInpux4_PIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = question_examples[1]\n",
        "answer = answer_examples[1]\n",
        "\n",
        "questionSearch, sel_sentence = demoProcess(question, paragraph, answer, lemma)\n",
        "totalFindAnswer = 0\n",
        "findCount = totalFindAnswer\n",
        "totalFindAnswer, process = demo(sel_sentence, answer, questionSearch, question, totalFindAnswer, 2, lemma)\n",
        "if totalFindAnswer != findCount:\n",
        "  print(process)\n",
        "  print('SNER: Answer found successfully, answer: ' + answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtYTk8pl_QwG",
        "outputId": "e16e4dc3-5a92-4ac0-dd78-56e5ea16ffb2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted sentence: [\"Find question:['was', 'Tarkan', 'born'], QCount:3\", 'Question rate:%75.0', '2. sentence - Tarkan was born on 17 October 1972 in Alzey, West Germany, to Neşe and Ali Tevetoğlu.', '', '', \"Find question:['was', 'Tarkan'], QCount:2\", 'Question rate:%50.0', \"4. sentence - Tarkan's first album, Yine Sensiz, was released on cassettes on 26 December 1992.\", '', '', \"Find question:['Tarkan'], QCount:1\", 'Question rate:%25.0', '1. sentence - Tarkan Tevetoğlu is a Turkish singer and songwriter.', '', '', \"Find question:['Tarkan'], QCount:1\", 'Question rate:%25.0', '3. sentence - The name Tarkan is said to originate from an ancient Turkic King.', '', '', '']\n",
            "Question's NER label: DATE\n",
            "\n",
            "Related sentence's NER labels:\n",
            "17 October 1972: DATE\n",
            "Alzey: ORG\n",
            "West Germany: GPE\n",
            "Ali Tevetoğlu: PERSON\n",
            "\n",
            "SNER: Answer found successfully, answer: 17 October 1972\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPOS example"
      ],
      "metadata": {
        "id": "9aL-yD2x_TKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = question_examples[2]\n",
        "answer = answer_examples[2]\n",
        "\n",
        "questionSearch, sel_sentence = demoProcess(question, paragraph, answer, lemma)\n",
        "totalFindAnswer = 0\n",
        "findCount = totalFindAnswer\n",
        "totalFindAnswer, process = demo(sel_sentence, answer, questionSearch, question, totalFindAnswer, 3, lemma)\n",
        "if totalFindAnswer != findCount:\n",
        "  print(process)\n",
        "  print('SPOS: Answer found successfully, answer:' + answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbcThTNA_W6p",
        "outputId": "bb9148f7-66b8-482d-b668-c73cab3333d0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted sentence: [\"Find question:['name', 'Tarkan', 'originate', 'from'], QCount:4\", 'Question rate:%66.66666666666666', '3. sentence - The name Tarkan is said to originate from an ancient Turkic King.', '', '', \"Find question:['Tarkan'], QCount:1\", 'Question rate:%16.666666666666664', '1. sentence - Tarkan Tevetoğlu is a Turkish singer and songwriter.', '', '', \"Find question:['Tarkan'], QCount:1\", 'Question rate:%16.666666666666664', '2. sentence - Tarkan was born on 17 October 1972 in Alzey, West Germany, to Neşe and Ali Tevetoğlu.', '', '', \"Find question:['Tarkan'], QCount:1\", 'Question rate:%16.666666666666664', \"4. sentence - Tarkan's first album, Yine Sensiz, was released on cassettes on 26 December 1992.\", '', '', '']\n",
            "Question's POS tag: NNP\n",
            "\n",
            "Removing question terms in sentence: name,Tarkan,originate,from,\n",
            "Related sentence's POS tags:\n",
            "The: DT\n",
            "is: VBZ\n",
            "said: VBD\n",
            "to: TO\n",
            "an: DT\n",
            "ancient: JJ\n",
            "Turkic: NNP\n",
            "King: NNP\n",
            ".: .\n",
            "\n",
            "SPOS: Answer found successfully, answer:Turkic King\n"
          ]
        }
      ]
    }
  ]
}